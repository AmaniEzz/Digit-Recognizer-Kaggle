{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":16,"outputs":[{"output_type":"stream","text":"/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n/kaggle/input/digit-recognizer/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the files \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers.core import Dense, Dropout, Activation\n\n\n\ntrain = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n#training data\nX_train  = train.drop(['label'], axis=1)\n#training labels\nY_target = train['label']\n\n#testing data\ntest  = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n\n#to free up some space\ndel train\n\n\n#print out the first 5 columns of train and test sets and their shapes\nprint(X_train.head())\nprint(np.shape(X_train))\nprint(test.head()) \nprint(np.shape(test))","execution_count":17,"outputs":[{"output_type":"stream","text":"   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n0       0       0       0       0       0       0       0       0       0   \n1       0       0       0       0       0       0       0       0       0   \n2       0       0       0       0       0       0       0       0       0   \n3       0       0       0       0       0       0       0       0       0   \n4       0       0       0       0       0       0       0       0       0   \n\n   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 784 columns]\n(42000, 784)\n   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n0       0       0       0       0       0       0       0       0       0   \n1       0       0       0       0       0       0       0       0       0   \n2       0       0       0       0       0       0       0       0       0   \n3       0       0       0       0       0       0       0       0       0   \n4       0       0       0       0       0       0       0       0       0   \n\n   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 784 columns]\n(28000, 784)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalizing the data (between 0 and 1) to help the CNN with the training\nX_train /= 255.0\ntest /= 255.0\n\n# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1) \n#chanal is 1 for grey scale images\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\n\n#Lets see the shape of X_train set and test set after normalizaing and reshaping\nprint(\"shape of training data after reshaping :-\" ,np.shape(X_train))\nprint(\"shape of testing data after reshaping :-\"  ,np.shape(test))","execution_count":18,"outputs":[{"output_type":"stream","text":"shape of training data after reshaping :- (42000, 28, 28, 1)\nshape of testing data after reshaping :- (28000, 28, 28, 1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding using keras (ex : 3 -> [0,0,0,1,0,0,0,0,0,0])\nprint(\"Labels Shape before one-hot encoding: \", Y_target.shape)\nY_target = to_categorical(Y_target, 10)\nprint(\"Labels Shape after one-hot encoding: \", Y_target.shape)","execution_count":19,"outputs":[{"output_type":"stream","text":"Labels Shape before one-hot encoding:  (42000,)\nLabels Shape after one-hot encoding:  (42000, 10)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the train and the testing set for the fitting\nX_train, X_test, Y_target, Y_test = train_test_split(X_train, Y_target, test_size = 0.1, random_state=0)\ng = plt.imshow(X_train[9][:,:,0])","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the CNN model \n\nmodel = Sequential()\n\n\n#First Hidden layer\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n#Dropout to avoid overfitting\nmodel.add(Dropout(0.25))\n\n#Second Hidden layer\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n#Dropout to avoid overfitting\nmodel.add(Dropout(0.25))\n\n#Flatten output of conv\nmodel.add(Flatten())\n\n#Fully Connected layer\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\n\n#Output layer\nmodel.add(Dense(10, activation = \"softmax\"))\n\n\n# compiling the sequential model\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n\n\n# training the model and saving metrics in history\nhistory = model.fit(X_train, Y_target,\n          batch_size=86, epochs=30,\n          verbose=2,\n          validation_data=(X_test, Y_test))","execution_count":26,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"add() got an unexpected keyword argument 'filters'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-39763cb95d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#First Hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Dropout to avoid overfitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: add() got an unexpected keyword argument 'filters'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,Y_target)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,Y_test)[1]*100 , \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the metrics\nfig = plt.figure()\nplt.subplot(2,1,1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\n\n\nplt.subplot(2,1,2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# predict results\nresults = model.predict(test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}